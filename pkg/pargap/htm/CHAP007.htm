<html><head><title>[ParGAP] 7 MPI commands and UNIX system calls in ParGAP</title></head>
<body text="#000000" bgcolor="#ffffff">
[<a href = "chapters.htm">Up</a>] [<a href ="CHAP006.htm">Previous</a>] [<a href ="CHAP008.htm">Next</a>] [<a href = "theindex.htm">Index</a>]
<h1>7 MPI commands and UNIX system calls in ParGAP</h1><p>
<P>
<H3>Sections</H3>
<oL>
<li> <A HREF="CHAP007.htm#SECT001">Tutorial introduction to the MPI C library</a>
<li> <A HREF="CHAP007.htm#SECT002">Other low level commands</a>
</ol><p>
<p>
This chapter can  be  safely  ignored  on  a  first  reading,  and  maybe
permanently. It is for application programmers who wish to develop  their
own low-level message-based parallel  application.  The  additional  UNIX
system calls in <font face="Gill Sans,Helvetica,Arial">ParGAP</font> may also be useful in some applications.
<p>
<p>
<a name="SECT001"><h2>7.1 Tutorial introduction to the MPI C library</h2></a>
<p><p>
<a name = "I0"></a>

<a name = "I1"></a>

<a name = "I2"></a>

<a name = "I2"></a>
<a name = "I3"></a>

This section lists some of the  more  common  message  passing  commands,
followed by a short MPI example.  The  next  section  (<a href="CHAP007.htm#SECT002">Other  low  level commands</a>) contains more (but by no means all) of the  MPI  commands  and
some UNIX system calls. The <font face="Gill Sans,Helvetica,Arial">ParGAP</font> binding provides a simplified  form
that makes interactive usage easier. This section describes the  original
MPI binding in&nbsp;C, with  some  comments  about  the  interactive  versions
provided in <font face="Gill Sans,Helvetica,Arial">ParGAP</font>. (The MPI standard includes a binding both to&nbsp;C and
to&nbsp;FORTRAN.)
<p>
Even if your ultimate goal is a standalone  C-based  application,  it  is
useful to prototype your application with  equivalent  commands  executed
interactively within <font face="Gill Sans,Helvetica,Arial">ParGAP</font>. Note that this  distribution  includes  a
subdirectory <code>mpinu</code>, which provides a  subset  MPI  implementation  in&nbsp;C
with a small <var>footprint</var>. It consists of a C&nbsp;library, <code>libmpi.a</code>, and  an
include  file  <code>mpi.h</code>.  The  library  is   approximately   150&nbsp;KB.   The
subdirectory can be consulted for further details.
<p>
We first briefly explain some MPI concepts.
<p>
<p>
<dl compact>
<p>
<dt><strong>rank</strong>:<dd> 
    The <strong>rank</strong> of an MPI process is a unique ID  number  associated  with
    the process. By convention, the console (master) process has  rank&nbsp;0.
    The ranks of the process are guaranteed by MPI to form a consecutive,
    ascending sequence of integers, starting with&nbsp;0.
<p>
<dt><strong>tag</strong>:<dd> 
    Each message has associated with  it  a  non-negative  integer  <strong>tag</strong>
    specified by the application. Our interface allows you to ignore tags
    by letting them take on default values. Typical application uses  for
    tags are either to choose consecutive integers in order to  guarantee
    that all messages can be re-assembled in sequence,  or  to  choose  a
    fixed set of constant tags, each  constant  associated  with  another
    type of message. In the latter case, one might have  integers  for  a
    <code>QUIT_TAG</code>, an <code>INTEGER_ARRAY_TAG</code>, <code>START_TASK2_TAG</code>, etc. In  fact,
    our  implementation  of  the  Slave  Listener   and   <code>MasterSlave()</code>
    specifically uses certain tags of value 1000  and  higher  for  these
    purposes.  Hence,  application  routines  that  do  use  tags  should
    restrict themselves to tags <code>[0..999]</code>.
<p>
<dt><strong>communicator</strong>:<dd> 
    A <strong>communicator</strong> in MPI serves the purpose of a namespace.  Most  MPI
    commands require a communicator argument to  specify  the  namespace.
    MPI  starts  up  with  a  default  namespace,  <code>MPI_COMM_WORLD</code>.  The
    <font face="Gill Sans,Helvetica,Arial">ParGAP</font> implementation always  assumes  that  single  namespace.  A
    namespace is important in MPI to build modules and library  routines,
    so that a thread may distinguish messages meant  for  itself,  or  to
    catch errors of cross-communication between two modules.
<p>
<dt><strong>message</strong>:<dd>
    Each message in MPI is typically implemented to  include  fields  for
    the source rank,  destination  rank  (optional),  tag,  communicator,
    count, and an array of data. The <var>count</var> field specifies  the  length
    of the array. MPI guarantees that messages are non-overtaking, in the
    sense that if two messages are sent from a single source  process  to
    the same destination process, then it is guaranteed  that  the  first
    process sent will be the first one to arrive, and will be received or
    probed first from the queue.
<p>
<dt><strong>other</strong>:<dd>
    MPI also has concepts of  <var>datatype</var>,  <var>derived  datatype</var>,  <var>group</var>,
    <var>topology</var>, etc. This implementation defaults those values,  so  that
    <var>datatype</var> is always  a  character  (hence  the  use  of  strings  in
    <font face="Gill Sans,Helvetica,Arial">ParGAP</font>), no <var>derived datatypes</var> are implemented, <var>group</var> is always
    consistent  with  <code>MPI_COMM_WORLD</code>,  and  <var>topology</var>  is  the   fully
    connected topology.
<p>
<dt><strong>communication</strong>:<dd>
    This  implementation  implements  only  point-to-point  communication
    (always blocking receives, except for <code>MPI_Iprobe</code>, and sends can  be
    blocking or not, according to the default underlying sockets).
<p>
<dt><strong>collective communication</strong>:<dd> 
    The MPI standard also provides for <strong>collective communication</strong>,  which
    sets up a barrier in which all process within the named  communicator
    must participate. One process is distinguished as the <strong>root</strong>  process
    in cases of  asymmetric  usage.  <font face="Gill Sans,Helvetica,Arial">ParGAP</font>  does  not  implement  any
    collective communication (although you can easily emulate it using  a
    sequence of point-to-point commands). The MPI subset distribution (in
    <font face="Gill Sans,Helvetica,Arial">ParGAP</font>'s  <code>mpinu</code>  directory)  does  provide  some  commands   for
    collective communication. Examples of  MPI  collective  communication
    commands are <code>MPI_Bcast</code> (broadcast), <code>MPI_Gather</code>  (place  an  entry
    from each  process  in  an  array  residing  on  the  root  process),
    <code>MPI_Scatter</code>  (inverse   of   gather),   <code>MPI_Reduce</code>   (execute   a
    commutative, associative function with an entry from each process and
    store on root; example functions are <code>sum</code>, <code>and</code>, <code>xor</code>, etc.
<p>
<dt><strong>dynamic processes</strong>:<dd> 
    The newer MPI-2 standard allows  for  the  dynamic  creation  of  new
    processes on new  processors  in  an  ongoing  MPI  computation.  The
    standard is silent on whether an MPI session should be aborted if one
    of its member processes  dies,  and  the  MPI  standard  provides  no
    mechanism to recognize such a dead process. Part of  the  reason  for
    this silence is that much of the ancestry of MPI  lies  in  dedicated
    parallel computers for which it would be unusual for one  process  or
    processor to die.
<p>
</dl>
<p>
Here is a short  extract  of  MPI  code  to  illustrate  its  flavor.  It
illustrates the C equivalents of the following <font face="Gill Sans,Helvetica,Arial">ParGAP</font>  commands.  Note
that the <font face="Gill Sans,Helvetica,Arial">ParGAP</font> versions noted here take fewer parameters  than  their
C-based cousins,  and  <font face="Gill Sans,Helvetica,Arial">ParGAP</font>  includes  defaults  for  some  optional
parameters.
<p>
<a name = "SSEC001.1"></a>
<li><code>MPI_Init() [ called for you automatically when ParGAP is loaded ] F</code>
<a name = "SSEC001.1"></a>
<li><code>MPI_Finalize() [ called for you automatically when GAP quits ] F</code>
<a name = "SSEC001.1"></a>
<li><code>MPI_Comm_rank() F</code>
<a name = "SSEC001.1"></a>
<li><code>MPI_Get_count() F</code>
<a name = "SSEC001.1"></a>
<li><code>MPI_Get_source() F</code>
<a name = "SSEC001.1"></a>
<li><code>MPI_Get_tag() F</code>
<a name = "SSEC001.1"></a>
<li><code>MPI_Comm_size() F</code>
<a name = "SSEC001.1"></a>
<li><code>MPI_Send( </code><var>string buf</var><code>, </code><var>int dest</var><code>[, </code><var>int tag = 0</var><code> ] ) F</code>
<a name = "SSEC001.1"></a>
<li><code>MPI_Recv( </code><var>string buf</var><code> [, </code><var>int source = MPI_ANY_SOURCE</var><code>[, </code><var>int tag = MPI_ANY_TAG</var><code> ]  ] ) F</code>
<a name = "SSEC001.1"></a>
<li><code>MPI_Probe( [ </code><var>int source = MPI_ANY_SOURCE</var><code>[, </code><var>int tag = MPI_ANY_TAG</var><code> ] ] ) F</code>
<p>
Many  of  the  above  commands  have  analogues  at  a  higher  level  in
section&nbsp;<a href="CHAP002.htm#SECT001">Slave    Listener     Commands</a>     as     <code>GetLastMsgSource()</code>,
<code>GetLastMsgTag()</code>, <code>MPI_Comm_size() = TOPCnumSlaves  +  1</code>,  <code>SendMsg()</code>,
<code>RecvMsg()</code> and <code>ProbeMsg()</code>.
<p>
<pre>
#include &lt;stdlib.h&gt;
#include &lt;mpi.h&gt;

#define MYCOUNT 5
#define INT_TAG 1

main( int argc, char *argv[] )
{
  int myrank;
  MPI_Init( &amp;argc, &amp;argv );
  MPI_Comm_rank( MPI_COMM_WORLD, &amp;myrank );

  if ( myrank == 0 ) {
    int mysize, dest, i;
    int buf;
    printf("My rank (master):  %d\n", myrank);
    for ( i=0; i&lt;MYCOUNT; i++ )
      buf = 5;
    MPI_Comm_size( MPI_COMM_WORLD, &amp;mysize );
    printf("Size:  %d\n", mysize);
    for ( dest=1; dest&lt; mysize; dest++ )
      MPI_Send( &amp;buf, MYCOUNT, MPI_INT, dest, INT_TAG, MPI_COMM_WORLD );
  } else {
    int i;
    MPI_Status status;
    int source;
    int count;
    int *buf;
    printf("My rank (slave):  %d\n", myrank);

    MPI_Probe( MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, &amp;status );
    printf( "Message pending with rank %d and tag %d.\n",
            status.MPI_SOURCE, status.MPI_TAG );
    if ( status.MPI_TAG != INT_TAG )
      printf("Error: Bad tag.\n"); exit(1);
    MPI_Get_count( &amp;status, MPI_INT, &amp;count );
    printf( "The count of how many data units (MPI_INT) is:  %d.\n", count );
    buf = (int *)malloc( count * sizeof(int) );

    source = status.MPI_SOURCE;
    MPI_Recv( buf, count, MPI_INT, source, INT_TAG, MPI_COMM_WORLD, &amp;status );
    for ( i=0; i&lt;MYCOUNT; i++ )
      if ( *buf != 5 ) printf("error:  buf[%d] != 5\n", i);
    printf("slave %d done.\n", myrank);
    }
  MPI_Finalize();
  exit(0);
}
</pre>
<p>
Even in this simplistic example, it was important to specify
<p>
<pre>
MPI_Recv( buf, count, MPI_INT, source, INT_TAG, MPI_COMM_WORLD, &amp;status );
</pre>
<p>
and not to use <code>MPI_ANY_SOURCE</code> instead of  the  known  source.  Although
this alternative would often work, there is a danger that there might  be
a second incoming message from a different source  that  arrives  between
the calls to <code>MPI_Probe()</code> and <code>MPI_Recv()</code>. In such an event, MPI  would
be free to receive the second message in <code>MPI_Recv()</code>,  even  though  the
appropriate count of the second message is likely to be  different,  thus
risking an overflow of the <code>buf</code> buffer.
<p>
Other typical bugs in MPI programs are:
<p>
<dl compact>
<p>
<dt><font face="symbol">·</font><dd>
    Incorrectly matching corresponding sends and receives or having  more
    or fewer sends than receives due to the logic of multiple  sends  and
    receives within distinct loops.
<p>
<dt><font face="symbol">·</font><dd>
    Reaching deadlock because all active processes have blocking calls to
    <code>MPI_Recv()</code> while no process has  yet  reached  code  that  executes
    <code>MPI_Send()</code>.
<p>
<dt><font face="symbol">·</font><dd>
    Incorrect use of barriers in collective  communication,  whereby  one
    process might execute:
<p>
<pre>
MPI_Send( buf, count, datatype, dest, tag, COMM_1 );
MPI_Bcast( buffer, count, datatype, root, COMM_2 );
</pre>
<p>
<dt><dd>
    and a second executes
<p>
<pre>
MPI_Bcast( buffer, count, datatype, root, COMM_2 );
MPI_Recv( buf, count, datatype, dest, tag, COMM_1, status );
</pre>
<p>
<dt><dd>
    If the call to <code>MPI_Send()</code> is blocking (as  is  the  case  for  long
    messages in the case of many implementations), then the first process
    will block at <code>MPI_Send()</code> while the second blocks at  'MPI_Bcast()'.
    This happens even though they use  distinct  communicators,  and  the
    send-receive communication  would  not  normally  interact  with  the
    broadcast communication.
<p>
</dl>
<p>
Much of the TOP-C method in <font face="Gill Sans,Helvetica,Arial">ParGAP</font> (see chapters&nbsp;<a href="CHAP003.htm">Basic  Concepts  for the TOP-C model (MasterSlave)</a> and&nbsp;<a href="CHAP005.htm">MasterSlave Tutorial</a>) was  developed
precisely to make errors like those above syntactically  impossible.  The
slave listener layer also does some additional work to keep track of  the
<code>status</code> that was last received and other bookkeeping. Additionally,  the
TOP-C method was  designed  to  provide  a  higher  level,  task-oriented
``language'', which would naturally lead the application programmer  into
designing an efficient high level algorithm.
<p>
<p>
<a name="SECT002"><h2>7.2 Other low level commands</h2></a>
<p><p>
<a name = "I4"></a>

<a name = "I5"></a>

Here is a complete  listing  of  the  low  level  commands  available  in
<font face="Gill Sans,Helvetica,Arial">ParGAP</font>.  Some  of  these  commands  were  documented  elsewhere.   The
remaining ones are not recommended for most users. Nevertheless,  it  may
be useful to others for more sophisticated applications.
<p>
For most of these commands, the source code is the ultimat documentation.
However, you may be able to guess at the meaning of many of them based on
their names and their similarity  UNIX  system  calls  (in  the  case  of
<code>UNIX_...</code>) or MPI commands (in  the  case  of  <code>MPI...</code>).  Some  of  the
commands will also show you their calling parameters if called  with  the
wrong number of arguments. Many  of  the  MPI  commands  have  simplified
calling parameters with certain arguments optional or  set  to  defaults,
making them easier for interactive use.
<p>
<a name = "I6"></a>

<a name = "I7"></a>

<a name = "SSEC002.1"></a>
<li><code>UNIX_MakeString( </code><var>len</var><code> ) F</code>
<a name = "SSEC002.1"></a>
<li><code>UNIX_DirectoryCurrent() [ Defined in `pkg/pargap/lib/slavelist.g</code> ] F</code>
<a name = "SSEC002.1"></a>
<li><code>UNIX_Chdir( </code><var>string</var><code> ) F</code>
<a name = "SSEC002.1"></a>
<li><code>UNIX_FflushStdout() F</code>
<a name = "SSEC002.1"></a>
<li><code>UNIX_Catch( </code><var>function</var><code>, </code><var>return_val</var><code> ) F</code>
<a name = "SSEC002.1"></a>
<li><code>UNIX_Throw() F</code>
<a name = "SSEC002.1"></a>
<li><code>UNIX_Getpid() F</code>
<a name = "SSEC002.1"></a>
<li><code>UNIX_Hostname() F</code>
<a name = "SSEC002.1"></a>
<li><code>UNIX_Alarm( </code><var>seconds</var><code> ) F</code>
<a name = "SSEC002.1"></a>
<li><code>UNIX_Realtime() F</code>
<a name = "SSEC002.1"></a>
<li><code>UNIX_Nice( </code><var>priority</var><code> ) F</code>
<a name = "SSEC002.1"></a>
<li><code>UNIX_LimitRss( </code><var>bytes_of_ram</var><code> ) [ = setrlimit(RLIMIT_RSS, ...) ] F</code>
<p>
<a name = "I8"></a>

<a name = "I9"></a>

<a name = "SSEC002.2"></a>
<li><code>MPI_Init() F</code>
<a name = "SSEC002.2"></a>
<li><code>MPI_Initialized() F</code>
<a name = "SSEC002.2"></a>
<li><code>MPI_Finalize() F</code>
<a name = "SSEC002.2"></a>
<li><code>MPI_Comm_rank() F</code>
<a name = "SSEC002.2"></a>
<li><code>MPI_Get_count() F</code>
<a name = "SSEC002.2"></a>
<li><code>MPI_Get_source() F</code>
<a name = "SSEC002.2"></a>
<li><code>MPI_Get_tag() F</code>
<a name = "SSEC002.2"></a>
<li><code>MPI_Comm_size() F</code>
<a name = "SSEC002.2"></a>
<li><code>MPI_World_size() F</code>
<a name = "SSEC002.2"></a>
<li><code>MPI_Error_string( </code><var>errorcode</var><code> ) F</code>
<a name = "SSEC002.2"></a>
<li><code>MPI_Get_processor_name() F</code>
<a name = "SSEC002.2"></a>
<li><code>MPI_Attr_get( </code><var>keyval</var><code> ) F</code>
<a name = "SSEC002.2"></a>
<li><code>MPI_Abort( </code><var>errorcode</var><code> ) F</code>
<a name = "SSEC002.2"></a>
<li><code>MPI_Send( </code><var>string buf</var><code>, </code><var>int dest</var><code>[, </code><var>int tag = 0</var><code> ] ) F</code>
<a name = "SSEC002.2"></a>
<li><code>MPI_Recv( </code><var>string buf</var><code> [, </code><var>int source = MPI_ANY_SOURCE</var><code>[, </code><var>int tag = MPI_ANY_TAG</var><code> ] ] ) F</code>
<a name = "SSEC002.2"></a>
<li><code>MPI_Probe( [ </code><var>int source = MPI_ANY_SOURCE</var><code>[, </code><var>int tag = MPI_ANY_TAG</var><code> ] ] ) F</code>
<a name = "SSEC002.2"></a>
<li><code>MPI_Iprobe( [ </code><var>int source = MPI_ANY_SOURCE</var><code>[, </code><var>int tag = MPI_ANY_TAG</var><code> ] ] ) F</code>
<p>
<a name = "I10"></a>

<a name = "I11"></a>

<a name = "SSEC002.3"></a>
<li><code>MPI_ANY_SOURCE V</code>
<a name = "SSEC002.3"></a>
<li><code>MPI_ANY_TAG V</code>
<a name = "SSEC002.3"></a>
<li><code>MPI_COMM_WORLD V</code>
<a name = "SSEC002.3"></a>
<li><code>MPI_TAG_UB V</code>
<a name = "SSEC002.3"></a>
<li><code>MPI_HOST V</code>
<a name = "SSEC002.3"></a>
<li><code>MPI_IO V</code>
<p>
<p>
[<a href = "chapters.htm">Up</a>] [<a href ="CHAP006.htm">Previous</a>] [<a href ="CHAP008.htm">Next</a>] [<a href = "theindex.htm">Index</a>]
<P>
<address>ParGAP manual<br>November 2001
</address></body></html>